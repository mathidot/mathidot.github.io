

---

layout : post
titile :数据整理
tags : 缺失的计算机课程

---

`journalctl | grep -i intel`，它会找到所有包含intel（不区分大小写）的系统日志。

数据处理不可缺少的两样东西：

* 用来整理的数据
* 相关的应用场景

日志处理通常是一个比较典型的使用场景，因为我们经常需要哎日志中查找某些信息，这种情况下通读日志是不现实的。现在，让我们研究一下系统日志，看看哪些用户尝试登陆过我们的服务器:

```shell
ssh myserver journalctl
```

内容太多了，现在把涉及`sshd`的信息过滤出来：

```shell
ssh myserver journalctl | grep sshd
```

这里我们使用管道将一个远程服务器上的文件传递给本机的`grep`程序！此时我们打印出来的内容，仍然比我们需要的要多得多，读起来非常费劲，我们改进一下：

```shell
ssh myserver 'journalctl | grep sshd | grep "Disconnected from"' | less
```

多出来的引号是什么作用呢？这么说吧，我们的日志是一个非常大的文件，把这么大的文件流直接传输到我们本地的电脑上在进行过滤是对流量的一种浪费。因此我们采取另外一种方式，我们先在远端机器上过滤文本内容，然后再将结果传输到本机。`less`为我们创建了一个文件分页器，使我们可以通过翻页的方式浏览较长的文本。为了进一步节省流量，我们甚至可以将当前过滤处的日志保存到文件中，这样后续就不需要再次通过网络访问该文件了：

```shell
ssh myserver 'journalctl | grep sshd | grep "Disconnected from"' > ssh.log
less ssh.log
```

过滤结果中仍然包含不少没用的数据，我们有很多办法可以删除这些无用的数据，但是让我们先研究一下`sed`这个非常强大的工具。

`sed`是一个基于文本编辑器`ed`构建的“流编辑器”。在`sed`中，你基本上是利用一些简短的命令来修改文件，而不是直接操作文件的内容。相关的命名非常多，但是最常用的是`s`，即替换命令。例如我们可以这样写：

```shell
ssh myserver journalctl
	| grep sshd
	| grep "Disconnected from"
	| sed 's/.*Disconnected from //'
```

上面这段命令中，我们使用了一段简单的正则表达式。正则表达式是一种非常强大的工具，可以让我们基于某种模式来对字符串进行匹配。`s`命令的语法如下：`s/REGEX/SUBSTITUTION`,其中`REGEX`部分是我们需要使用的正则表达式，而`SUBSTITUTION`是用于替换匹配结果的文本

## 正则表达式

- `.` 除换行符之外的”任意单个字符”
- `*` 匹配前面字符零次或多次
- `+` 匹配前面字符一次或多次
- `[abc]` 匹配 `a`, `b` 和 `c` 中的任意一个
- `(RX1|RX2)` 任何能够匹配`RX1` 或 `RX2`的结果
- `^` 行首
- `$` 行尾

`sed`的正则表达式有些时候是比较奇怪的，它需要你在这些模式前添加`\`才能使其具有特殊含义。或者，你页可以添加`-E`选项来支持这些匹配。

回过头来看`/.*Disconnected from /`，我们会发现这个正则表达式可以匹配任意以若干字符开头，并接着包含`Disconnected from`的字符串，这正是我们希望的。但是如果有人将“Disconnected from”作为自己的用户名呢？

```shell
Jan 17 03:13:00 thesquareplanet.com sshd[2631]: Disconnected from invalid user Disconnected from 46.97.239.16 port 55920 [preauth]
```

正则式会如何匹配？`*`和`+`在默认情况下是贪婪模式，也就是说，它们会尽可能多的匹配文本。因此对上述字符串的匹配结果如下：

```shell
46.97.239.16 port 55920 [preauth]
```

这可不是我们想要的结果。对于某些正则表达式的实现来说，你可以给`*`或`+` 增加一个`?` 后缀使其变成非贪婪模式，但是很可惜 `sed` 并不支持该后缀。不过，我们可以切换到 perl 的命令行模式，该模式支持编写这样的正则表达式：

```shell
perl -pe 's/.*?Disconnected from //'
```

让我们回到 `sed` 命令并使用它完成后续的任务，毕竟对于这一类任务，`sed`是最常见的工具。`sed` 还可以非常方便的做一些事情，例如打印匹配后的内容，一次调用中进行多次替换搜索等。但是这些内容我们并不会在此进行介绍。`sed` 本身是一个非常全能的工具，但是在具体功能上往往能找到更好的工具作为替代品。

好的，我们还需要去掉用户名的后缀，应该如何操作呢？

想要匹配用户名后面的文本，尤其是当这里的用户名可以包含空格时，这个问题变得非常棘手！这里我们需要做的是匹配一整行：

```shell
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user .* [^ ]+ port [0-9]+( \[preauth\])?$//'
```

让我们借助正则表达式在线调试工具[regex debugger](https://regex101.com/r/qqbZqh/2) 来理解这段表达式。OK，开始的部分和以前是一样的，随后，我们匹配两种类型的“user”（在日志中基于两种前缀区分）。再然后我们匹配属于用户名的所有字符。接着，再匹配任意一个单词（`[^ ]+` 会匹配任意非空且不包含空格的序列）。紧接着后面匹配单“port”和它后面的一串数字，以及可能存在的后缀`[preauth]`，最后再匹配行尾。

注意，这样做的话，即使用户名是“Disconnected from”，对匹配结果也不会有任何影响，您知道这是为什么吗？

问题还没有完全解决，日志的内容全部被替换成了空字符串，整个日志的内容因此都被删除了。我们实际上希望能够将用户名*保留*下来。对此，我们可以使用“捕获组（capture groups）”来完成。被圆括号内的正则表达式匹配到的文本，都会被存入一系列以编号区分的捕获组中。捕获组的内容可以在替换字符串时使用（有些正则表达式的引擎甚至支持替换表达式本身），例如`\1`、 `\2`、`\3`等等，因此可以使用如下命令：

```shell
| sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
```

## 回到数据整理

OK，现在我们有如下表达式：

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/
```

`sed` 还可以做很多各种各样有趣的事情，例如文本注入：(使用 `i` 命令)，打印特定的行 (使用 `p`命令)，基于索引选择特定行等等。详情请见`man sed`!

现在，我们已经得到了一个包含用户名的列表，列表中的用户都曾经尝试过登陆我们的系统。但这还不够，让我们过滤出那些最常出现的用户：

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
```

`sort` 会对其输入数据进行排序。`uniq -c` 会把连续出现的行折叠为一行并使用出现次数作为前缀。我们希望按照出现次数排序，过滤出最常出现的用户名：

`sort -n` 会按照数字顺序对输入进行排序（默认情况下是按照字典序排序 `-k1,1` 则表示“仅基于以空格分割的第一列进行排序”。`,n` 部分表示“仅排序到第n个部分”，默认情况是到行尾。就本例来说，针对整个行进行排序也没有任何问题，我们这里主要是为了学习这一用法！

如果我们希望得到登陆次数最少的用户，我们可以使用 `head` 来代替`tail`。或者使用`sort -r`来进行倒序排序。

相当不错。但我们只想获取用户名，而且不要一行一个地显示。

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10
 | awk '{print $2}' | paste -sd,
```

我们可以利用 `paste`命令来合并行(`-s`)，并指定一个分隔符进行分割 (`-d`)，那`awk`的作用又是什么呢？

## awk – 另外一种编辑器

`awk` 其实是一种编程语言，只不过它碰巧非常善于处理文本。关于 `awk` 可以介绍的内容太多了，限于篇幅，这里我们仅介绍一些基础知识。

首先， `{print $2}` 的作用是什么？ `awk` 程序接受一个模式串（可选），以及一个代码块，指定当模式匹配时应该做何种操作。默认当模式串即匹配所有行（上面命令中当用法）。 在代码块中，`$0` 表示整行的内容，`$1` 到 `$n` 为一行中的 n 个区域，区域的分割基于 `awk` 的域分隔符（默认是空格，可以通过`-F`来修改）。在这个例子中，我们的代码意思是：对于每一行文本，打印其第二个部分，也就是用户名。

让我们康康，还有什么炫酷的操作可以做。让我们统计一下所有以`c` 开头，以 `e` 结尾，并且仅尝试过一次登陆的用户。

```shell
 | awk '$1 == 1 && $2 ~ /^c[^ ]*e$/ { print $2 }' | wc -l
```

让我们好好分析一下。首先，注意这次我们为 `awk`指定了一个匹配模式串（也就是`{...}`前面的那部分内容）。该匹配要求文本的第一部分需要等于1（这部分刚好是`uniq -c`得到的计数值），然后其第二部分必须满足给定的一个正则表达式。代码块中的内容则表示打印用户名。然后我们使用 `wc -l` 统计输出结果的行数。

不过，既然 `awk` 是一种编程语言，那么则可以这样：

```shell
BEGIN { rows = 0 }
$1 == 1 && $2 ~ /^c[^ ]*e$/ { rows += $1 }
END { print rows }
```

`BEGIN` 也是一种模式，它会匹配输入的开头（ `END` 则匹配结尾）。然后，对每一行第一个部分进行累加，最后将结果输出。事实上，我们完全可以抛弃 `grep` 和 `sed` ，因为 `awk` 就可以[解决所有问题](https://backreference.org/2010/02/10/idiomatic-awk)。至于怎么做，就留给读者们做课后练习吧。

## 分析数据

想做数学计算也是可以的！例如这样，您可以将每行的数字加起来：

```shell
 | paste -sd+ | bc -l
```

下面这种更加复杂的表达式也可以：

```shell
echo "2*($(data | paste -sd+))" | bc -l
```

您可以通过多种方式获取统计数据。如果已经安装了R语言，[`st`](https://github.com/nferraz/st)是个不错的选择：

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | awk '{print $1}' | R --slave -e 'x <- scan(file="stdin", quiet=TRUE); summary(x)'
```

R 也是一种编程语言，它非常适合被用来进行数据分析和[绘制图表](https://ggplot2.tidyverse.org/)。这里我们不会讲的特别详细， 您只需要知道`summary` 可以打印某个向量的统计结果。我们将输入的一系列数据存放在一个向量后，利用R语言就可以得到我们想要的统计数据。

如果您希望绘制一些简单的图表， `gnuplot` 可以帮助到您：

```shell
ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10
 | gnuplot -p -e 'set boxwidth 0.5; plot "-" using 1:xtic(2) with boxes'
```

## 利用数据整理来确定参数

有时候您要利用数据整理技术从一长串列表里找出你所需要安装或移除的东西。我们之前讨论的相关技术配合 `xargs` 即可实现：

```shell
rustup toolchain list | grep nightly | grep -vE "nightly-x86" | sed 's/-x86.*//' | xargs rustup toolchain uninstall
```

## 整理二进制数据

虽然到目前为止我们的讨论都是基于文本数据，但对于二进制文件其实同样有用。例如我们可以用 ffmpeg 从相机中捕获一张图片，将其转换成灰度图后通过SSH将压缩后的文件发送到远端服务器，并在那里解压、存档并显示。

```shell
ffmpeg -loglevel panic -i /dev/video0 -frames 1 -f image2 -
 | convert - -colorspace gray -
 | gzip
 | ssh mymachine 'gzip -d | tee copy.jpg | env DISPLAY=:0 feh -'
```



# 课后习题

1. 统计words文件 (`/usr/share/dict/words`) 中包含至少三个`a` 且不以`'s` 结尾的单词个数

   ```shell
   cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | wc -l
   ```

   - 大小写转换：`tr "[:upper:]" "[:lower:]"`

   - `^([^a]*a){3}.*[^'s]$`：查找一个以 a 结尾的字符串三次

   - ```shell
     grep -v "\'s$"
     ```

     ：匹配结尾为’s 的结果，然后取反。 借助

     ```shell
     grep -v
     ```

     主要是这里不支持 lookback，不然下面的正则就可以完成

     ```shell
      ^([^a]*a){3}.*(?<!'s)$
     ```

这些单词中，出现频率前三的末尾两个字母是什么？

```shell
cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*([a-z]{2})$/\1/" | sort | uniq -c | sort | tail -n3
# 53 as
# 64 ns
# 102 an
```

共存在多少种词尾两字母组合？

```shell
cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*([a-z]{2})$/\1/" | sort | uniq | wc -l
```

还有一个很 有挑战性的问题：哪个组合从未出现过？ 为了得到没出现的组合，首先我们要生成一个包含全部组合的列表，然后再使用上面得到的出现的组合，比较二者不同即可。

```shell
#!/bin/bash
for i in {a..z};do
 for j in {a..z};do
    echo  "$i$j"
 done
done
```

```shell
./all.sh > all.txt
```

```shell
cat /usr/share/dict/words | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" | sed -E "s/.*([a-z]{2})$/\1/" | sort | uniq > occurance.txt
```

```shell
diff --unchanged-group-format='' <(cat occurance.txt) <(cat all.txt) | wc -l
```

`--unchanged-group-format=''`用于将两个文件中相同的内容设置为空字符串，剩下的内容就是差异的部分。

2. 进行原地替换听上去很有诱惑力，
3. 例如： `sed s/REGEX/SUBSTITUTION/ input.txt > input.txt`。但是这并不是一个明智的做法，为什么呢？还是说只有 `sed`是这样的? 查看 `man sed` 来完成这个问题。`sed s/REGEX/SUBSTITUTION/ input.txt > input.txt` 表达式中后一个 `input.txt`会首先被清空，而且是发生在前的。所以前面一个`input.txt`在还没有被 `sed` 处理时已经为空了。在使用正则处理文件前最好是首先备份文件。

```shell
sed -i.bak s/REGEX/SUBSTITUTION/ input.txt > input.txt
```

可以自动创建一个后缀为 `.bak` 的备份文件。

3. 找出您最近十次开机的开机时间平均数、中位数和最长时间。在Linux上需要用到 `journalctl`.

   ```shell
   (base) mathidot3@ubuntu:~$ journalctl --list-boots
   -18 7a71263f226348489b058a6a65ec2ff7 Thu 2021-12-09 22:48:10 PST—Fri 2021-12-10 00:43:41 PST
   -17 59a41e35608c4992ac78160b60413a3c Fri 2021-12-10 00:43:57 PST—Fri 2021-12-10 19:08:35 PST
   -16 ddb9b3f4f6b844f48be66b4238ddc5cf Sat 2021-12-11 23:27:14 PST—Tue 2021-12-14 02:50:41 PST
   -15 c7109030f0f74e1aa028ed77b60e6bda Tue 2021-12-14 02:51:01 PST—Tue 2021-12-14 03:03:27 PST
   -14 3ccab151510a4b9eb87273ae3e2db931 Tue 2021-12-14 03:30:57 PST—Tue 2021-12-14 03:30:57 PST
   -13 874a8620a7654c98805dd25d4dafb207 Thu 2021-12-30 01:26:56 PST—Thu 2021-12-30 02:25:30 PST
   -12 d4fee1a83a9048c68ff680741aedeb21 Thu 2021-12-30 02:38:51 PST—Thu 2021-12-30 17:44:49 PST
   -11 2f7e6df538f04fb0be62452d64d2e3b5 Thu 2021-12-30 17:45:03 PST—Thu 2021-12-30 19:00:18 PST
   -10 a4ce615cfde64bf6ae67e6cf01a04580 Thu 2021-12-30 19:05:49 PST—Fri 2021-12-31 20:55:19 PST
    -9 22f8b781dfe5488da791523103c74435 Fri 2022-05-06 19:04:15 PDT—Fri 2022-05-06 19:06:45 PDT
    -8 d2d6f585bc44469799aaf1923863e3e2 Fri 2022-05-06 19:26:13 PDT—Sat 2022-05-07 17:22:14 PDT
    -7 2df6ecc7902e47dab08ed644cc2d8dea Wed 2022-05-25 23:20:44 PDT—Fri 2022-05-27 19:56:48 PDT
    -6 115b9cc92b614ddfb19ef1dc868738b9 Sat 2022-05-28 17:06:34 PDT—Sat 2022-05-28 17:29:40 PDT
    -5 0bb641a79b9c416f8990d256c2141dce Sat 2022-05-28 17:29:55 PDT—Sat 2022-05-28 19:17:01 PDT
    -4 20962d3d3d844507949b05c0a39c0eff Sun 2022-05-29 17:57:46 PDT—Tue 2022-05-31 04:00:36 PDT
    -3 9dc479d0086a4c30af0c788f2b79784c Tue 2022-05-31 04:00:58 PDT—Tue 2022-05-31 04:36:29 PDT
    -2 cfecda8e5ed2494a9d85ada96357a7bf Tue 2022-05-31 04:49:32 PDT—Thu 2022-06-02 03:10:09 PDT
    -1 3d5439b6cac044ef94f0a42786b36faf Thu 2022-06-02 03:10:28 PDT—Thu 2022-06-16 19:41:50 PDT
     0 f3a22684297641e68e1735f6d64e2a53 Sun 2022-06-19 07:17:51 PDT—Mon 2022-06-20 06:53:32 PDT
   
   ```

   可以使用 systemd-analyze工具看一下启动时间都花在哪里：

   ```shell
   sudo systemd-analyze plot > systemd.svg
   ```

![1.png](https://missing-semester-cn.github.io/missing-notes-and-solutions/2020/solutions/images/4/3.svg)

可以看到启动时间为 14.157s。 接下来，编写脚本getlog.sh来获取最近十次的启动时间数据

```shell
#!/bin/bash
for i in {0..9}; do
   journalctl -b-$i | grep "Startup finished in"
done
```

```shell
./getlog > starttime.txt
```

```shell
#获取最长时间
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort --numeric-sort | tail -n1
#获取最短时间
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort -r --numeric-sort | tail -n1
#平均数（注意 awk 要使用单引号）
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| paste -sd+ | bc -l | awk '{print $1/10}'
# 中位数
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort |paste -sd\  | awk '{print ($5+$6)/2}'
```

如果配合使用 R 语言脚本则更加简单：

```shell
sudo apt-get install r-base
```

```shell
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort | R -e 'd<-scan("stdin",quiet=TRUE);min(d);max(d);mean(d);median(d);'
```

```shell
> d<-scan("stdin",quiet=TRUE);min(d);max(d);mean(d);median(d);
[1] 14.023
[1] 15.989
[1] 14.4304
[1] 14.2915
```

